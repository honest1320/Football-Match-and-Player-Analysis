{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate \n",
    "from langchain_core.runnables import RunnableMap, RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the Dataset and inspect it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Data preprocessing. Here you can add what you think might be useful to have a better seasonal analysis. Examples:\n",
    " - over / under performance indicators\n",
    " - normalizations\n",
    " - percentiles \n",
    " - rankings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. Write a prompt template to extract the informations you need, so player full name and season indication. \n",
    "\n",
    "Your goal here is to have the model generating a structured output, from which you can parse the relevant information (dict, list, comma separated string...)\n",
    "\n",
    "Optional: To have a more flexible app that could also compare players, think of way to extract multiple players at once\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLAYER_INFO_TEMPLATE = '''\n",
    "\n",
    "\n",
    "#TODO\n",
    "'''\n",
    "\n",
    "\n",
    "player_info_template = PromptTemplate(\n",
    "        template=PLAYER_INFO_TEMPLATE, \n",
    "        input_variables=#TODO\n",
    "        )\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', timeout=4, max_retries=3, temperature=0)\n",
    "\n",
    "info_chain = \"\"#TODO: PIPE THE PROMPT TEMPLATE INTO THE LLM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Test the extraction step! Look at the output and then find a way to parse it into a python data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: write a input prompt and test the first step\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Extract text from AI Message object and transform it into a data structure usable in pyhton. Check the output data type!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result, type(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Filter the player you want to analyze (select the correct season!) from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = #TODO FILTER DATASET "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Check your filtered df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Transform your filtered dataframe into a list of dict (Hint: .to_dict(orient='records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players_data = filtered_df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Write a prompt template to generate the final report given:\n",
    "- user input\n",
    "- extracted data\n",
    "- metrics descriptions (you can load them from  the script utils/descriptions.py). Are these input variables? or not?\n",
    "\n",
    "Use the prompt techniques described into the lecture if you think they might fit. There is no right or wrong solution here.\n",
    "Prompt engineering is a trial and error process, try different things and be creative!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.descriptions import Descriptions\n",
    "OUTPUT_TEMPLATE = '''\n",
    "#TODO WRITE THE OUTPUT PROMPT TEMPLATE.\n",
    "'''\n",
    "\n",
    "#TODO ADD VARIABLES HERE. HINT: YOU NEED MORE VARIABLES\n",
    "output_template = PromptTemplate(\n",
    "\n",
    "        )\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', timeout=30, max_retries=3, temperature=0)\n",
    "\n",
    "output_chain = output_template | llm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Invoke the chain and see results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "report = #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(report.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Make all togheter in a single chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_player_info(info_to_parse):\n",
    "    #TODO: THIS FUNCTION MUST RETURN THE PARSED CONTENT FROM THE FIRST CALL\n",
    "    return parsed_info\n",
    "\n",
    "def filter_and_format_data(parsed_info)->dict:\n",
    "    #TODO THIS FUNCTION MUST OUTPUT THE FINAL DATA YOU WILL PASS TO THE SECOND STEP\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's chain all togheter and invoke all with a single call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal_report_chain = (RunnableMap(               #Runnable map is used to redirect the input variable \"input\" both to the player_info_template and the output_template\n",
    "            steps={\n",
    "                    \"input\": lambda x: x['input'],  #the \"input\" variable is passed to the next step without any modification\n",
    "                    \"data\": (                       #while the \"data\" variable is derived runtime with a subchain\n",
    "                        player_info_template \n",
    "                        | llm \n",
    "                        | parse_player_info \n",
    "                        | filter_and_format_data\n",
    "                    ),\n",
    "                })\n",
    "            | RunnableLambda(lambda x: x['steps'])\n",
    "            | output_template\n",
    "            | llm \n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO TEST THE FINAL CHAIN WITH AN INPUT\n",
    "res = seasonal_report_chain.invoke({'input': })\n",
    "pprint(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
